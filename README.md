# MachineLearningNotes
## 机器学习是什么

+ 机器学习致力于研究如何通过计算的手段，利用经验来改善自身的性能。在计算机系统中，“经验”通常以“数据“形式存在。因此，机器学习所研究的主要内容，是关于在计算机上从数据产生”模型“的算法，即”学习算法“。有了学习算法，我们把经验数据提供给它，它就能基于这些数据产生模型；在面对新的情况时，模型会给我们提供相应的判例。

## 机器学习的基本概念

+ 数据集
+ 属性（特征）/属性空间（样本空间）
+ 真相：数据潜在的自身规律
+ 假设：学得模型对应了关于数据的某种潜在的规律
+ 训练集：训练集通常只是样本空间的一个很小的采样（但仍希望它能很好地反映出样本空间的特性）
+ 测试集
+ 有监督学习
  -分类任务： 预测离散值
  -回归任务： 预测连续值
+ 无监督学习：
  -聚类任务 
+ 泛化（能力）：学得模型适用与新样本的能力，具有强泛化能力的模型能很好的适用于整个样本空间。

## 机器学习的基本方法论

1. 学习到底是在做什么？

我们可以把学习过程看作一个在所有假设组成的空间中进行搜索的过程，搜索目标是找到与训练集”匹配“的假设。

2. 怎么去搜索假设？

可以有许多策略对这个假设空间进行搜索（学习的方法），最终获得与训练集一致的假设（学习的结果）

3. 搜索到的与训练集一致的假设会有多个吗？

现实中，通常面临很大的假设空间，学习过程是基于有限样本训练集进行的，因此，可能有多个假设与训练集一致，也就是存在一个”假设集合“

4. 如果存在”假设集合“怎么办，挑选哪个？

如果存在多个与训练集一致的假设，但它们对应的模型，当面临新样本时，会产生不同的输出，那么应该选哪个模型（假设）呢？
对于一个具体的学习算法而言，它必须要产生一个模型，这时，学习算法本身的”偏好“就会起到关键作用。例如，算法喜欢”尽可能特殊“还是”尽可能通用“
一般性的原则，来引导算法确立”正确的“偏好，*奥卡姆剃刀(Occam's razor)*是一种常用的的原则，即”若有多个假设与观察一致，则选最简单的那个“

5. 奥卡姆剃刀原则就一定是对的吗？它有没有什么问题？

有的时候，判断哪个更”简单“，这个事情并不简单，需要借助其他机制才能解决

6. 根据偏好挑出的算法，是否能在所有的实际问题中都表现比其他的好？

很遗憾，对于一个学习算法A，若它在某些问题上比学习算法B好，那么必然存在另一些问题，在那里B比A好。
甚至，当假设所有问题出现的机会相同、或所有问题同等重要的条件下，所有的学习算法的期望性能都跟随机猜测差不多。

7. 既然跟随机猜差不多，那还有必要进行机器学习吗？

实际上，生活中面临的问题，并不一定是均匀分布（即上述的假设），因此，脱离具体的问题，空谈”什么学习算法更好“毫无意义
